{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Første forsøk på sammenmosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a cell we must run from the start because the robot's nvargus daemon acts up and throws camera errors like nobody's business.\n",
    "\n",
    "import os\n",
    "password = \"jetbot\"\n",
    "command = \"sudo -S systemctl restart nvargus-daemon\"\n",
    "os.system('echo %s | %s' % (password, command))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are writing in a seperate folder we need to add paths to the other modules not contained in the current system paths.\n",
    "import sys\n",
    "sys.path.insert(1,'/home/jetbot')\n",
    "sys.path.insert(1,'/home/jetbot/monodepth2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jetson.utils.__init__.py\n",
      "jetson.inference.__init__.py\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function  # unknown what this is but everything fails if i remove it.\n",
    "%matplotlib inline \n",
    "\n",
    "# Utility functions for type conversions \n",
    "from jetson.utils import cudaFromNumpy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as Fun\n",
    "\n",
    "\n",
    "# Monodepth\n",
    "from monodepth2 import *\n",
    "import monodepth2.networks as networks\n",
    "from monodepth2.utils import download_model_if_doesnt_exist\n",
    "\n",
    "\n",
    "# CAMERA\n",
    "from jetbot import Camera, Robot\n",
    "\n",
    "# Preprocessing\n",
    "import cv2\n",
    "import time\n",
    "import PIL.Image as pil\n",
    "\n",
    "# Source of Object Detection Net\n",
    "from jetson import inference\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# If GPU is available, we will use it for efficiency\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camspecs are to specifications:  True\n",
      "320 240\n"
     ]
    }
   ],
   "source": [
    "# Instanciate camera with defined size.\n",
    "camwidth, camheight = 320, 240 #224,224\n",
    "cam = Camera.instance(width=camwidth, height=camheight)\n",
    "\n",
    "try:\n",
    "    print(\"Camspecs are to specifications: \", ((cam.width == camwidth) and (cam.height == camheight)))\n",
    "    print(cam.width, cam.height)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Instanciate Camera\n",
    "robot = Robot()\n",
    "robot.stop() # Sometimes the robot start driving when instanciated, so this is in case it does this.\n",
    "\n",
    "def geometric_average(l:list):\n",
    "    '''\n",
    "    This function returns the geometric average of the given list\n",
    "    '''\n",
    "    return (sum(np.array(l)**2)/len(l))**(1/2)\n",
    "\n",
    "# We predefine the general constants used later for efficiency in the future cells.\n",
    "total = []\n",
    "blocking_threshold = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OF Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate our Detection Network from the Jetson Inference package with the model name and the threshold level.\n",
    "net = inference.detectNet(\"ssd-mobilenet-v2\", threshold=0.4)\n",
    "\n",
    "\n",
    "# We define the Detection bool as false to start.\n",
    "det = False\n",
    "accepted_classes = [47]  # classID from coco_labels.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up CA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In thsi block we create and customize our Collision avoidance model.\n",
    "model = torchvision.models.alexnet(pretrained=False) # Load Alexnet as our model\n",
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2) #last layer wants 1000 labels, we only need 2, so we swap last layer with a linear binary layer.\n",
    "model.load_state_dict(torch.load('best_model.pth')) # Load with our best trained model\n",
    "model = model.to(device) # Push model onto superior GPU\n",
    "\n",
    "# Standard Imagenet normalization\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "\n",
    "def preprocess(camera_value): # Preprocessing for camera format --> Network input format\n",
    "    \"\"\"\n",
    "    Preprocessing function for camera format --> network input format\n",
    "    \n",
    "    param camera_value: input from Camera\n",
    "    type camera_value: np.ndarray\n",
    "    \"\"\"\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB) # For some unknown reason the camera input is on BGR format, so we change it into RGBA.\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Monodepth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 640\n",
      "240 192\n"
     ]
    }
   ],
   "source": [
    "# We build our monocular depth estimation model from the Monodepth module\n",
    "\n",
    "# Define which model to use and download if not found\n",
    "model_name = \"mono_640x192\"\n",
    "download_model_if_doesnt_exist(model_name)\n",
    "\n",
    "# Build paths to coders and instanciate from path\n",
    "encoder_path = os.path.join(\"models\", model_name, \"encoder.pth\")\n",
    "depth_decoder_path = os.path.join(\"models\", model_name, \"depth.pth\")\n",
    "encoder = networks.ResnetEncoder(18, False).cuda()\n",
    "depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4)).cuda()\n",
    "\n",
    "# Encoder and Decoder loading\n",
    "loaded_dict_enc = torch.load(encoder_path, map_location='cpu')\n",
    "filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "encoder.load_state_dict(filtered_dict_enc)\n",
    "loaded_dict = torch.load(depth_decoder_path, map_location='cpu')\n",
    "depth_decoder.load_state_dict(loaded_dict)\n",
    "\n",
    "# Put the coders in evaluation mode\n",
    "encoder.eval()\n",
    "depth_decoder.eval();\n",
    "\n",
    "#pull out feed size to use for processing in loop, do now for efficiency in execution \n",
    "feed_height = loaded_dict_enc['height']\n",
    "feed_width = loaded_dict_enc['width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CA Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CA_update(change):\n",
    "    \"\"\"\n",
    "    This function receives the change in the camera values and acts on it if the confidence of a blocked path is above the blocking threshold.\n",
    "    This function forms the basis of the collision avoidance safety for the monocular depth perception.\n",
    "    \n",
    "    param change: The change observed in the camera\n",
    "    type change: Dictionary\n",
    "    \"\"\"\n",
    "    global robot, prob_blocked\n",
    "    x = change['new'] \n",
    "    x = preprocess(x)\n",
    "    y = model(x)\n",
    "    \n",
    "    # we apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    y = Fun.softmax(y, dim=1)\n",
    "    \n",
    "    # If the blocking confidence is higher than the set threshold the smart collision avoidance overrides the motor functions of the monocular depth estimator.\n",
    "    prob_blocked = float(y.flatten()[0])\n",
    "    if prob_blocked > blocking_threshold: \n",
    "        # The CA aspect chooses the turn based on its previous motor values. The basic concept being a smooth turn is the one continuing on the already progressing one. \n",
    "        if robot.left_motor.value > robot.right_motor.value:\n",
    "            robot.right(0.4)\n",
    "        else:\n",
    "            robot.left(0.4)\n",
    "        \n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OD Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OD_update(change):\n",
    "    \"\"\"\n",
    "    This function receives the change in the camera values and runs object detection. If a detection of the desired class is made,\n",
    "    OD overrides the normal functionality to track down the detected object.\n",
    "    This function forms the basis of the endgame override to the monocular depth perception.\n",
    "    \n",
    "    param change: The change observed in the camera\n",
    "    type change: Dictionary\n",
    "    \"\"\"\n",
    "    global robot, cam, det\n",
    "    try: # Preprocessing of input values\n",
    "        arr = change['new'] \n",
    "        frame = cv2.cvtColor(arr, cv2.COLOR_BGR2RGBA) # input is on RBGR, network needs RGBA.\n",
    "        camCuda = cudaFromNumpy(frame)\n",
    "        detections = net.Detect(image = camCuda, width = camwidth, height = camheight, overlay = \"none\")\n",
    "        det = False # We set detection flag to false.\n",
    "        for d in detections:\n",
    "            if d.ClassID in accepted_classes:\n",
    "                print(\"OD ACTIVATED\")\n",
    "                center_x, center_y = d.Center\n",
    "                det = True\n",
    "                break\n",
    "        if det: # if we have detected the desired object, we enter into object following.\n",
    "            if d.Height/camheight > 0.25:\n",
    "                robot.stop()\n",
    "                cam.unobserve_all()\n",
    "                print(\"Hider Located! Victory!\")\n",
    "            else:\n",
    "                x = center_x/camwidth - 0.5 \n",
    "                robot.set_motors(\n",
    "                    float(0.4 + 0.8 * x), # left\n",
    "                    float(0.4 - 0.8 * x)) # Right\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OD ACTIVATED\n",
      "OD ACTIVATED\n",
      "Hider Located! Victory!\n"
     ]
    }
   ],
   "source": [
    "CA_update({'new': cam.value}) # We instanciate the update functions with the current camera values\n",
    "OD_update({'new': cam.value})\n",
    "cam.observe(CA_update, names='value') # Now we connect the update functions to the camera. This wil cause the update functions o be called whenever we have a change in the camera values.\n",
    "cam.observe(OD_update, names='value')\n",
    "while True: # Run indefinitely, only ended when we have success or we have an resignation by KeyboardInterrupt.\n",
    "    try:\n",
    "        if det:\n",
    "            cam.unobserve(CA_update, names='value')\n",
    "            break\n",
    "        elif prob_blocked < blocking_threshold:\n",
    "            # preprocessing: \n",
    "            inputImage = pil.fromarray(cam.value.astype('uint8'),'RGB') #.rotate(180)\n",
    "            input_image_resized = inputImage.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "            input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0).cuda()\n",
    "\n",
    "            # MD net\n",
    "            with torch.no_grad():\n",
    "                features = encoder(input_image_pytorch)\n",
    "                outputs = depth_decoder(features)\n",
    "            disp = outputs[(\"disp\", 0)]\n",
    "            \n",
    "            # postprocessing\n",
    "            disp_resized = torch.nn.functional.interpolate(disp.cpu(), (camheight, camwidth), mode=\"bilinear\", align_corners=False).cuda()\n",
    "            disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Algorithm for evaluating depth perceptions and pushing onto motor values\n",
    "            A = []\n",
    "            B = []\n",
    "            C = []\n",
    "            edge1 = round(camwidth/3)\n",
    "            edge2 = round(2*camwidth/3)\n",
    "            for w in range(camheight):\n",
    "                for j in range(edge1):\n",
    "                    A.append(disp_resized_np[w][j])\n",
    "                for j in range(edge1, edge2):\n",
    "                    B.append(disp_resized_np[w][j])\n",
    "                for j in range(edge2, camwidth):\n",
    "                    C.append(disp_resized_np[w][j])\n",
    "            B_scale = 1 - geometric_average(B)/np.max(disp_resized_np)\n",
    "            right_motor = 2*(0.5 - geometric_average(A)*B_scale)\n",
    "            left_motor = 2*(0.5 - geometric_average(C)*B_scale)\n",
    "            robot.set_motors(left_motor, right_motor)           \n",
    "    except KeyboardInterrupt: # Set up a keyboard interrupt so that if we abort from keyboard it will dismantle properly and not SIGKILL.\n",
    "        cam.unobserve_all()\n",
    "        robot.stop()\n",
    "        print('Loop successfully ended, loop')\n",
    "        break\n",
    "        \n",
    "# We end up here if we successfully find our object, therefore we cant dismantle all camera observables and for safety also stop robot.\n",
    "cam.unobserve_all()\n",
    "robot.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
